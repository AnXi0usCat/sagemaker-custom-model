{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load lightgbm/train.py\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "import lightgbm as lgb\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import traceback\n",
    "\n",
    "# default values for hyper parameters \n",
    "hyper_params_default = {\n",
    "    'num_leaves': 12, \n",
    "    'objective': 'binary', \n",
    "    'metric': 'auc', \n",
    "    'seed': 7,\n",
    "    'num_class': 2,\n",
    "    'learning_rate': 0.01,\n",
    "    \"verbose\": -1\n",
    "}\n",
    "\n",
    "# default values for training parameters\n",
    "training_params_default = {\n",
    "    'num_boost_round': 3000,\n",
    "    'early_stopping_rounds': 10\n",
    "}\n",
    "\n",
    "# default types for hyper parameters \n",
    "hyper_params_default_types = {\n",
    "    'num_leaves': int, \n",
    "    'objective': str, \n",
    "    'metric': str, \n",
    "    'seed': int,\n",
    "    'num_class': int,\n",
    "    'learning_rate': float,\n",
    "    \"verbose\": int\n",
    "}\n",
    "\n",
    "# default types for training parameters\n",
    "training_params_default_types = {\n",
    "    'num_boost_round': int,\n",
    "    'early_stopping_rounds': int\n",
    "}\n",
    "\n",
    "# paths where sagemaker does it's business\n",
    "prefix = '/opt/ml/'\n",
    "input_path =  os.path.join(prefix, 'input/data')\n",
    "output_path = os.path.join(prefix, 'output')\n",
    "model_path = os.path.join(prefix, 'model')\n",
    "param_path = os.path.join(prefix, 'input/config/hyperparameters.json')\n",
    "\n",
    "\n",
    "# specify where we should get the data\n",
    "training_channel_name = 'training'\n",
    "validation_channel_name = 'validation'\n",
    "training_path = os.path.join(input_path, training_channel_name)\n",
    "validation_path = os.path.join(input_path, validation_channel_name)\n",
    "\n",
    "\n",
    "def convert_to_dataset(df):\n",
    "    x = df.iloc[:, 1:]\n",
    "    y = df.iloc[:, 0]\n",
    "    return lgb.Dataset(x, label=y)\n",
    "\n",
    "\n",
    "def load_data_from_files(paths, channel):\n",
    "        input_files = [os.path.join(paths, file) for file in os.listdir(paths)]\n",
    "        if len(input_files) == 0:\n",
    "            raise ValueError(('There are no files in {}.\\n' +\n",
    "                              'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                              'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                              'does not have permission to access the data.').format(paths, channel))\n",
    "        raw_data = [pd.read_csv(file, header=None) for file in input_files]\n",
    "        data_df = pd.concat(raw_data)\n",
    "        return data_df\n",
    "\n",
    "\n",
    "def parse_parameters(params):\n",
    "    # only work with copies so we dont override the truth\n",
    "    hyper_params = hyper_params_default.copy()\n",
    "    training_params = training_params_default.copy()\n",
    "    \n",
    "    for par in params:\n",
    "        # if the hyper parameter is supported then we replace our default values\n",
    "        # with provided value after converting it to the required type\n",
    "        # becasue in SageMaker everyhting is passed as strings\n",
    "        if par in hyper_params:\n",
    "            hyper_params[par] = hyper_params_default_types[par](params[par])\n",
    "\n",
    "        if par in training_params:\n",
    "            training_params[par] = training_params_default_types[par](params[par])\n",
    "  \n",
    "    return hyper_params, training_params\n",
    "\n",
    "\n",
    "def train():\n",
    "    print('Starting training model: LightGBM')\n",
    "    try:\n",
    "        # get the hyperparam values\n",
    "        with open(param_path, 'r') as tc:\n",
    "            training_params = json.load(tc)\n",
    "        hyper_p, training_p = parse_parameters(training_params)\n",
    "\n",
    "        # training and validation data\n",
    "        train = load_data_from_files(training_path, training_channel_name)\n",
    "        valid = load_data_from_files(validation_path, validation_channel_name)\n",
    "        \n",
    "        # convert into format which lightgbm understands\n",
    "        dtrain = convert_to_dataset(train)\n",
    "        dvalid = convert_to_dataset(valid)\n",
    "        \n",
    "        model = lgb.train(hyper_p, \n",
    "                          dtrain, \n",
    "                          valid_sets=[dvalid],\n",
    "                          num_boost_round=training_p['num_boost_round'], \n",
    "                          early_stopping_rounds=training_p['early_stopping_rounds'], \n",
    "                          verbose_eval=True)\n",
    "        \n",
    "        # save the trained model\n",
    "        with open(os.path.join(model_path, 'light-gbm-model.pkl'), 'wb') as out:\n",
    "            pickle.dump(model, out)\n",
    "        print('Training complete.')   \n",
    "    except Exception as e:\n",
    "        # Write out an error file. This will be returned as the failureReason in the\n",
    "        # DescribeTrainingJob result.\n",
    "        trc = traceback.format_exc()\n",
    "        with open(os.path.join(output_path, 'failure'), 'w') as s:\n",
    "            s.write('Exception during training: ' + str(e) + '\\n' + trc)\n",
    "        # Printing this causes the exception to be in the training job logs, as well.\n",
    "        print('Exception during training: ' + str(e) + '\\n' + trc, file=sys.stderr)\n",
    "        # A non-zero exit code causes the training job to be marked as Failed.\n",
    "        sys.exit(255)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()\n",
    "    # A zero exit code causes the job to be marked a Succeeded.\n",
    "    sys.exit(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pickle\n",
    "\n",
    "model_path = 'local/testing/model/'\n",
    "\n",
    "class Predictor(object):\n",
    "    model = None\n",
    "       \n",
    "    @classmethod\n",
    "    def get_model(cls):\n",
    "        \"\"\"\n",
    "        Load the model file from the model directory\n",
    "        \"\"\"\n",
    "        if cls.model is None:\n",
    "            with open(os.path.join(model_path, 'light-gbm-model.pkl'), 'r') as inp:\n",
    "                cls.model = pickle.load(inp)\n",
    "        return cls.model\n",
    "    \n",
    "    @classmethod\n",
    "    def predict(cls, input):\n",
    "        \"\"\"\n",
    "        Make a prediction with the lightgbm model\n",
    "        \"\"\"\n",
    "        if cls.model is None:\n",
    "            cls.model = cls.get_model()\n",
    "        \n",
    "        return model.predict(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-f422e7d8d96d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mPredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-c25d11ba17b6>\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'light-gbm-model.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                 \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "Predictor.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
